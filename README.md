promptfoo
---

`promptfoo` is a CLI tool that facilitates prompt editing and evaluation by providing a systematic approach to comparing LLM outputs. With the ability to run a test suite on multiple prompts, catch regressions, and analyze results, `promptfoo` offers a practical solution for evaluating model outputs.

## Features

- Test multiple prompts against predefined test cases
- Compare LLM outputs in a structured manner
- Support for OpenAI API models
- Customizable API provider integration
- CSV input/output for seamless data handling and processing

By incorporating these features, `promptfoo` enables users to make more informed decisions when refining prompts and assessing language model performance.

## Usage

To evaluate prompts using `promptfoo`, use the following command:

```bash
npx promptfoo eval -p <prompt_paths...> -o <output_path> -r <provider> [-v <vars_path>]
```

- `<prompt_paths...>`: Paths to prompt files
- `<output_path>`: Path to output CSV file
- `<provider>`: One of: `openai:chat`, `openai:completion`, `openai:chat:<model_name>`, `openai:completion:<model_name>`, or filesystem path to custom API caller module
- `<vars_path>` (optional): Path to CSV file with prompt variables


### Prompt Files

Prompt files are plain text files that contain the prompts you want to test. If you have only one file, you can include multiple prompts in the file, separated by the delimiter `---`. If you have multiple files, each prompt should be in a separate file. You can use [Nunjucks](https://mozilla.github.io/nunjucks/) templating syntax to include variables in your prompts, which will be replaced with actual values from the `vars` CSV file during evaluation.

Example of a single prompt file with multiple prompts (`prompts.txt`):

```
Translate the following English text to French: "{{text}}"
---
What is the capital of France?
```

Example of multiple prompt files:

- `prompt1.txt`:

  ```
  Translate the following English text to French: "{{text}}"
  ```

- `prompt2.txt`:

  ```
  What is the capital of France?
  ```

### Vars Files

Vars files are CSV, JSON, or YAML files that contain the values for the variables used in the prompts. The first row of the CSV file should contain the variable names, and each subsequent row should contain the corresponding values for each test case.

Example of a vars file (`vars.csv`):

```
text
"Hello, world!"
"Goodbye, everyone!"
```

Example of a vars file (`vars.json`):

```json
[
  {"text": "Hello, world!"},
  {"text": "Goodbye, everyone!"}
]
```

### Output File

The output file is a CSV file that contains the results of the evaluation. Each row in the output file corresponds to a test case and includes the original prompt, the output generated by the LLM, and the values of the variables used in the test case.

Example of an output file (`results.csv`):

```
Prompt,Output,text
"Translate the following English text to French: ""{{text}}""","Bonjour, le monde !","Hello, world!"
"Translate the following English text to French: ""{{text}}""","Au revoir, tout le monde !","Goodbye, everyone!"
```

Example of an output file (`results.json`):

```json
[
  {
    "Prompt": "Translate the following English text to French: \"{{text}}\"",
    "Output": "Bonjour, le monde !",
    "text": "Hello, world!"
  },
  {
    "Prompt": "Translate the following English text to French: \"{{text}}\"",
    "Output": "Au revoir, tout le monde !",
    "text": "Goodbye, everyone!"
  }
]
```

### Example

```bash
promptfoo eval -p prompt1.txt prompt2.txt -o results.csv -r openai:chat -v vars.csv
```

This command will evaluate the prompts in `prompt1.txt` and `prompt2.txt` using the OpenAI chat API, subtituing the variable values from `vars.csv`, and save the results to `results.csv`.

## Installation

1. Clone the repository:

```bash
git clone https://github.com/typpo/promptfoo.git
```

2. Install the dependencies:

```bash
npm install
```

3. Link the CLI tool:

```bash
npm link
```

### Example

```bash
promptfoo eval -p prompt1.txt prompt2.txt -o results.csv -r openai:chat -v vars.csv
```

## API Providers

`promptfoo` supports OpenAI API models out of the box. To use a custom API provider, create a custom module that implements the `ApiProvider` interface and pass the path to the module as the `provider` option.

### OpenAI API

To use the OpenAI API, set the `OPENAI_API_KEY` environment variable or pass the API key as an argument to the constructor.

Example:

```bash
export OPENAI_API_KEY=your_api_key_here
```


### Custom API Provider

To create a custom API provider, implement the `ApiProvider` interface in a separate module. Here is the interface:

```javascript
export interface ApiProvider {
  callApi: (prompt: string) => Promise<ProviderResult>;
}
```

Below is an example of a custom API provider that returns a predefined output and token usage:

```javascript
// customApiProvider.js
class CustomApiProvider {
  async callApi(prompt) {
    // Add your custom API logic here

    return {
      // Required
      output: 'Model output',

      // Optional
      tokenUsage: {
        total: 10,
        prompt: 5,
        completion: 5,
      },
    };
  }
}

module.exports.default = CustomApiProvider;
```

To use the custom API provider with `promptfoo`, pass the path to the module as the `provider` option in the CLI invocation:

```bash
promptfoo eval -p prompt1.txt prompt2.txt -o results.csv -r ./customApiProvider.js -v vars.csv
```

This command will evaluate the prompts using the custom API provider and save the results to the specified CSV file.

## Developing

Contributions are welcome! Please feel free to submit a pull request or open an issue.

`promptfoo` includes several npm scripts to make development easier and more efficient. To use these scripts, run `npm run <script_name>` in the project directory.

Here are some of the available scripts:

- `build`: Transpile TypeScript files to JavaScript
- `watch`: Continuously watch and transpile TypeScript files on changes
- `test`: Run test suite
- `test:watch`: Continuously run test suite on changes
